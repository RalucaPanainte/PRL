{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFopYyg5Q9wq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "iGGgKbewRVv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, ignore_mismatched_sizes=True)\n",
        "\n",
        "def fetch_messages_llama(messages, model=model, tokenizer=tokenizer, temperature=0):\n",
        "    # Prepare input for the model\n",
        "    input_text = messages[-1]['content']\n",
        "    inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=1024, temperature=temperature)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def refresh_conversation_llama(chat, context):\n",
        "    context.append({'role': 'user', 'content': f\"{chat}\"})\n",
        "    response = fetch_messages_llama(context, temperature=0.7)\n",
        "    context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
        "    return response\n",
        "\n",
        "def main():\n",
        "    results = {}\n",
        "\n",
        "    # Initialize the context with interaction rules and product data\n",
        "    context = []\n",
        "\n",
        "    # Define the chatbot's interaction rules here, including how it should greet users, provide extra information.\n",
        "    rules = \"\"\"For this task, you'll be asked to annotate album review sentences from the Pitchfork website. Before describing the task, let's have a quick look at Pitchfork. Pitchfork is a website publishing music reviews of mainly rock, but also folk, heavy metal, electronic music and hip-hop albums.\n",
        "    For each sentence group, follow these instructions :\n",
        "    Carefully read the text of the review, paying close attention to details. Be careful not to use shortcuts: for example, a positive phrase does not necessarily mean creativity.\n",
        "    Classify the phrase group as creative (1), uncreative (2) or indifferent (3)\n",
        "    Phrases should be coded as CREATIVE when they claim that the music of the artist or band is creative. This includes references to innovation within a music genre, bridging between several music genres, risk-taking, artistic openness, and technical innovation.\n",
        "    Phrases should be coded as UNCREATIVE when they evoke a lack of creativity on the part of the artist or group. This includes mentions of a blatant lack of innovation. Mentions of a lack of depth in lyrics or production. And finally, mentions of an impression of copying or déjà-vu.\n",
        "    Sentence groups should be coded as INDIFFERENT when they don't fit into any of these categories.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the product data from the CSV file\n",
        "    with open(\"creative_validation2.csv\", encoding='utf-8', errors='ignore') as file:\n",
        "        input_data = file.read()\n",
        "\n",
        "    context.append({'role': 'system', 'content': f\"\"\"{rules} {input_data}\"\"\"})\n",
        "\n",
        "    for run in range(3):\n",
        "        result = refresh_conversation_llama(\"You were provided with a database, and your task is to classify each sentence as creative, uncreative, or indifferent. Please provide the id of the sentence along with the corresponding label.\", context)\n",
        "\n",
        "        # Split the result\n",
        "        split_result = result.split('\\n')\n",
        "\n",
        "        # Processing stage\n",
        "        for i in split_result:\n",
        "            part = i.strip().split(', ')\n",
        "            if len(part) == 2:\n",
        "                id_, label = part[0], part[1]\n",
        "                if id_ not in results:\n",
        "                    results[id_] = [id_, '', '', '']\n",
        "                results[id_][run + 1] = label\n",
        "\n",
        "    # Output the results in the CSV file\n",
        "    with open('output_all_runs.csv', 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        csvwriter.writerow(['ID', 'Label 1', 'Label 2', 'Label 3'])\n",
        "        for row in results.values():\n",
        "            csvwriter.writerows([row])\n",
        "\n",
        "    with open(\"output_all_runs.csv\") as file:\n",
        "        input_data = file.read()\n",
        "\n",
        "    # Split data into lines\n",
        "    lines = input_data.strip().split('\\n')\n",
        "\n",
        "    # Create a dictionary to store IDs and their corresponding labels\n",
        "    id_labels = {}\n",
        "\n",
        "    # Extract IDs and labels\n",
        "    for line in lines:\n",
        "        entries = line.split(',')\n",
        "        for i in range(1, len(entries)):\n",
        "            if entries[0]:  # Check if ID exists\n",
        "                id_ = entries[0]\n",
        "                label = entries[i]\n",
        "                if id_ not in id_labels:\n",
        "                    id_labels[id_] = []\n",
        "                if label:\n",
        "                    id_labels[id_].append(label)\n",
        "\n",
        "    with open('output.csv', 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        csvwriter.writerow(['ID', 'Label'])\n",
        "        # Finding the majority label\n",
        "        for id_, labels in id_labels.items():\n",
        "            if labels:  # Check if there are labels for this ID\n",
        "                majority_label = Counter(labels).most_common(1)[0][0]\n",
        "                csvwriter.writerow([id_, majority_label])\n",
        "\n",
        "    # Load ground truth labels\n",
        "    with open(\"creative_validation2_ground_truth.csv\") as file:\n",
        "        ground_truth_data = file.readlines()\n",
        "\n",
        "    # Load predicted labels\n",
        "    with open(\"output.csv\") as file:\n",
        "        predicted_data = file.readlines()\n",
        "\n",
        "    # Initialize lists to store ground truth and predicted labels\n",
        "    ground_truth_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    # Process ground truth and predicted data\n",
        "    for ground_truth_line, predicted_line in zip(ground_truth_data, predicted_data):\n",
        "        _, ground_truth_label = ground_truth_line.strip().split(',')\n",
        "        _, predicted_label = predicted_line.strip().split(',')\n",
        "        ground_truth_labels.append(ground_truth_label)\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    # Removing the first element since its the header \"Label\"\n",
        "    ground_truth_labels.pop(0)\n",
        "    predicted_labels.pop(0)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "    precision = precision_score(ground_truth_labels, predicted_labels, average='macro')\n",
        "    recall = recall_score(ground_truth_labels, predicted_labels, average='macro')\n",
        "    f1 = f1_score(ground_truth_labels, predicted_labels, average='macro')\n",
        "    cross_tab = pd.crosstab(ground_truth_labels, predicted_labels)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(cross_tab)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "CFDRj-CCRWvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}